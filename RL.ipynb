{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Juuuuuuuuun/RL-Term-Project/blob/main/RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# metadata.widgets ì‚­ì œ (ì§ì ‘ í¸ì§‘)\n",
        "import json\n",
        "\n",
        "# í˜„ì¬ ë…¸íŠ¸ë¶ íŒŒì¼ëª…ì„ ë„£ì–´ì¤€ë‹¤\n",
        "nb_file = \"/content/drive/MyDrive/Colab Notebooks/RL.ipynb\"\n",
        "\n",
        "with open(nb_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    nb = json.load(f)\n",
        "\n",
        "# widgets metadata ì œê±°\n",
        "if \"widgets\" in nb[\"metadata\"]:\n",
        "    del nb[\"metadata\"][\"widgets\"]\n",
        "\n",
        "with open(nb_file.replace(\".ipynb\", \"_clean.ipynb\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(nb, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(\"í´ë¦° ë²„ì „ ìƒì„± ì™„ë£Œ:\", nb_file.replace(\".ipynb\", \"_clean.ipynb\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yyph-iXb5mk-",
        "outputId": "e745a354-b526-4ccb-ff7b-3ff0b28151c9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "í´ë¦° ë²„ì „ ìƒì„± ì™„ë£Œ: /content/drive/MyDrive/Colab Notebooks/RL_clean.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1V_-71vObHk",
        "outputId": "79d4e3e9-2dc4-481b-f0bf-6fef037260ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: gymnasium==0.29.1 in /usr/local/lib/python3.12/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.29.1) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.29.1) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.29.1) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.29.1) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# ============================================\n",
        "# ğŸ“Œ 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •\n",
        "# ============================================\n",
        "# - transformers: BERT ì„ë² ë”© ì¶”ì¶œìš©\n",
        "# - datasets: HF datasets (ë³¸ í”„ë¡œì íŠ¸ì—ì„œëŠ” ì‚¬ìš© X, but RL í”„ë¡œì íŠ¸ ê¸°ë³¸ ì„¸íŒ…)\n",
        "# - torch: PyTorch (ëª¨ë¸ í•™ìŠµ)\n",
        "# - gymnasium: ê°•í™”í•™ìŠµ í™˜ê²½ êµ¬í˜„\n",
        "# ============================================\n",
        "\n",
        "!pip install transformers datasets torch gymnasium==0.29.1\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from tqdm import tqdm\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# ============================================\n",
        "# ğŸ“Œ 2. GPU ì‚¬ìš© ì—¬ë¶€ ìë™ í™•ì¸\n",
        "# ============================================\n",
        "# Colabì—ì„œ GPUê°€ ì¼œì ¸ ìˆìœ¼ë©´ ëª¨ë¸/BERT forward ì†ë„ ë§¤ìš° ë¹ ë¥´ê²Œ ìˆ˜í–‰ ê°€ëŠ¥.\n",
        "# ============================================\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "BfF7wRrvG9dY"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# ğŸ“Œ 3. ë°ì´í„° ë¡œë“œ\n",
        "# ============================================\n",
        "# - encoding='utf-8' ë˜ëŠ” cp949 í•„ìš” (í•œêµ­ì–´ CSV íŒŒì¼ì€ cp949ì¸ ê²½ìš° ë§ìŒ)\n",
        "# ============================================\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/df.csv', encoding='utf-8')\n",
        "\n",
        "# ============================================\n",
        "# ğŸ“Œ 4. ì»¬ëŸ¼ëª… í‘œì¤€í™”\n",
        "# ============================================\n",
        "# GOODS_NAME â†’ goods\n",
        "# GOODS_OPTN_VLU â†’ option\n",
        "# MODEL_NAME â†’ answer\n",
        "# RL í™˜ê²½ì´ ì½ê¸° ì‰½ê²Œ ì»¬ëŸ¼ ì´ë¦„ì„ í†µì¼í•¨.\n",
        "# ============================================\n",
        "\n",
        "df = df.rename(columns={\n",
        "    \"GOODS_NAME\": \"goods\",\n",
        "    \"GOODS_OPTN_VLU\": \"option\",\n",
        "    \"MODEL_NAME\": \"answer\"\n",
        "})\n",
        "\n",
        "df[\"goods\"] = df[\"goods\"].fillna(\"\").astype(str)\n",
        "df[\"option\"] = df[\"option\"].fillna(\"\").astype(str)\n",
        "\n",
        "# labels: ì •ë‹µ í›„ë³´ ë¦¬ìŠ¤íŠ¸ â†’ í˜„ì¬ëŠ” ì •ë‹µ 1ê°œë¡œ êµ¬ì„±\n",
        "df[\"labels\"] = df[\"answer\"].apply(lambda x: [x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEWzW8zOPKDQ",
        "outputId": "fef7215f-a40c-4c72-efec-ef909f42c7aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertModel(\n",
              "  (embeddings): Embeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (transformer): Transformer(\n",
              "    (layer): ModuleList(\n",
              "      (0-5): 6 x TransformerBlock(\n",
              "        (attention): DistilBertSdpaAttention(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (ffn): FFN(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (activation): GELUActivation()\n",
              "        )\n",
              "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# ============================================\n",
        "# ğŸ“Œ 5. BERT ëª¨ë¸ ë¡œë“œ (DistilBERT)\n",
        "# ============================================\n",
        "# - í…ìŠ¤íŠ¸ì—ì„œ ì˜ë¯¸ ì„ë² ë”©(768ì°¨ì›)ì„ ì¶”ì¶œ\n",
        "# - ì´í›„ ê°•í™”í•™ìŠµì—ì„œëŠ” ì´ ì„ë² ë”©ë§Œ ì‚¬ìš©í•˜ì—¬ ì†ë„ ëŒ€í­ í–¥ìƒ\n",
        "# - ëª¨ë¸ì€ GPUë¡œ ì´ë™ (.to(device))\n",
        "# ============================================\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"distilbert-base-uncased\").to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cS0rkwHPZIA",
        "outputId": "db2b5fc2-155b-4c7e-d33d-ef80fa58bde8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1048575/1048575 [1:23:53<00:00, 208.30it/s]\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# ğŸ“Œ 6. í…ìŠ¤íŠ¸ë¥¼ DistilBERT ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜\n",
        "# ============================================\n",
        "# input: \"ìƒí’ˆëª… + ì˜µì…˜\" ë¬¸ìì—´\n",
        "# output: 768ì°¨ì› CLS vector\n",
        "# ============================================\n",
        "\n",
        "@torch.no_grad()\n",
        "def embed_text(text):\n",
        "    tokens = tokenizer(text, return_tensors=\"pt\",\n",
        "                       truncation=True, padding=False, max_length=64).to(device)\n",
        "    output = model(**tokens).last_hidden_state[:, 0, :]  # CLS í† í°\n",
        "    return output.squeeze().cpu().numpy()\n",
        "\n",
        "# ============================================\n",
        "# ğŸ“Œ 7. ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ ì‚¬ì „ ì„ë² ë”© ìƒì„± (ì¤‘ìš”)\n",
        "# ============================================\n",
        "# - RL í™˜ê²½ ë‚´ì—ì„œ BERTë¥¼ ë§¤ë²ˆ ì‹¤í–‰í•˜ë©´ ë§¤ìš° ëŠë¦¼\n",
        "# - ë”°ë¼ì„œ ëª¨ë“  ë°ì´í„°ë¥¼ ë¯¸ë¦¬ embeddingí•˜ì—¬ ë©”ëª¨ë¦¬ì— ì €ì¥\n",
        "# ============================================\n",
        "\n",
        "emb_list = []\n",
        "\n",
        "for text in tqdm(df[\"goods\"] + \" \" + df[\"option\"], total=len(df)):\n",
        "    emb = embed_text(text)\n",
        "    emb_list.append(emb)\n",
        "\n",
        "df[\"embedding\"] = emb_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "z9OfV-OHGX7F"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# ğŸ“Œ 8. ê°•í™”í•™ìŠµ í™˜ê²½ ì •ì˜ (Gym Environment)\n",
        "# ============================================\n",
        "# - State: ì‚¬ì „ ê³„ì‚°ëœ BERT ì„ë² ë”© (768ì°¨ì›)\n",
        "# - Action: max_candidates í¬ê¸° (ê¸°ë³¸ 10ê°œ)\n",
        "# - Reward: ì •ë‹µ ì„ íƒ ì‹œ +1, ì˜¤ë‹µ ì„ íƒ ì‹œ -1\n",
        "# - EpisodeëŠ” Single-step êµ¬ì¡° (ë¬¸ì¥ í•˜ë‚˜ë¥¼ ë³´ê³  ë°”ë¡œ ì„ íƒ)\n",
        "# ============================================\n",
        "\n",
        "class ModelExtractEnv(gym.Env):\n",
        "    def __init__(self, df, max_candidates=10):\n",
        "        super().__init__()\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.n = len(df)\n",
        "        self.max_candidates = max_candidates\n",
        "\n",
        "        # RL action space (ì •ë‹µ í›„ë³´ ì¤‘ í•˜ë‚˜ ì„ íƒ)\n",
        "        self.action_space = spaces.Discrete(max_candidates)\n",
        "\n",
        "        # observation_space = BERT embedding 768ì°¨ì›\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-5, high=5, shape=(768,), dtype=np.float32\n",
        "        )\n",
        "\n",
        "        self.index = 0\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        # ëœë¤í•˜ê²Œ í•˜ë‚˜ì˜ ìƒí’ˆ ì„ íƒ\n",
        "        self.index = np.random.randint(0, self.n)\n",
        "        row = self.df.iloc[self.index]\n",
        "\n",
        "        # ë°”ë¡œ ì‚¬ì „ ê³„ì‚°ëœ embedding ì‚¬ìš© â†’ ë§¤ìš° ë¹ ë¦„\n",
        "        obs = row[\"embedding\"]\n",
        "\n",
        "        return obs, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        row = self.df.iloc[self.index]\n",
        "        labels = row[\"labels\"]\n",
        "        answer = row[\"answer\"]\n",
        "\n",
        "        # ì •ë‹µ íŒë‹¨\n",
        "        reward = 1.0 if action < len(labels) and labels[action] == answer else -1.0\n",
        "\n",
        "        terminated = True  # Single-step MDP\n",
        "\n",
        "        # ë‹¤ìŒ ìƒíƒœëŠ” ì˜ë¯¸ ì—†ìŒ â†’ zero vector ë°˜í™˜\n",
        "        return np.zeros((768,), dtype=np.float32), reward, terminated, False, {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "RmbTxiYZIF2_"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# ğŸ“Œ 9. Actor-Critic ê¸°ë°˜ ëª¨ë¸ ì •ì˜\n",
        "# ============================================\n",
        "# - PPO, A2Cì—ì„œ ê³µìš©ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë‰´ëŸ´ë„·\n",
        "# - Actor: ì •ì±… Ï€(a|s)\n",
        "# - Critic: ìƒíƒœ ê°€ì¹˜ í•¨ìˆ˜ V(s)\n",
        "# ============================================\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, obs_dim, action_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # ì •ì±… ë„¤íŠ¸ì›Œí¬ (actor)\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(obs_dim, 256), nn.ReLU(),\n",
        "            nn.Linear(256, action_dim), nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "        # ê°€ì¹˜ ë„¤íŠ¸ì›Œí¬ (critic)\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(obs_dim, 256), nn.ReLU(),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.actor(x), self.critic(x)\n",
        "\n",
        "# ============================================\n",
        "# ğŸ“Œ 10. PPO Agent ì •ì˜\n",
        "# ============================================\n",
        "# - clipped objective ì‚¬ìš©\n",
        "# - ì•ˆì •ì ì¸ policy ì—…ë°ì´íŠ¸\n",
        "# ============================================\n",
        "\n",
        "class PPOAgent:\n",
        "    def __init__(self, obs_dim, action_dim, lr=3e-4, gamma=0.99, eps_clip=0.2):\n",
        "        self.policy = ActorCritic(obs_dim, action_dim)\n",
        "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "\n",
        "    # í–‰ë™ ì„ íƒ\n",
        "    def act(self, obs):\n",
        "        obs = torch.tensor(obs, dtype=torch.float32)\n",
        "        probs, _ = self.policy(obs)\n",
        "        dist = Categorical(probs)\n",
        "        action = dist.sample()\n",
        "        return action.item(), dist.log_prob(action).detach()\n",
        "\n",
        "    # PPO ì—…ë°ì´íŠ¸\n",
        "    def update(self, memory):\n",
        "        obs = torch.stack(memory[\"obs\"]).detach()\n",
        "        actions = torch.tensor(memory[\"actions\"])\n",
        "        rewards = torch.tensor(memory[\"rewards\"]).detach()\n",
        "        logprobs_old = torch.stack(memory[\"logprobs\"]).detach()\n",
        "\n",
        "        # Return ê³„ì‚°\n",
        "        returns = []\n",
        "        R = 0\n",
        "        for r in reversed(rewards.tolist()):\n",
        "            R = r + self.gamma * R\n",
        "            returns.insert(0, R)\n",
        "        returns = torch.tensor(returns, dtype=torch.float32)\n",
        "\n",
        "        for _ in range(5):  # ì—¬ëŸ¬ epoch ë°˜ë³µ\n",
        "            probs, values = self.policy(obs)\n",
        "            dist = Categorical(probs)\n",
        "            logprobs_new = dist.log_prob(actions)\n",
        "\n",
        "            ratio = torch.exp(logprobs_new - logprobs_old)\n",
        "            advantage = (returns - values.squeeze()).detach()\n",
        "\n",
        "            surr1 = ratio * advantage\n",
        "            surr2 = torch.clamp(ratio, 1-self.eps_clip, 1+self.eps_clip)*advantage\n",
        "\n",
        "            loss = -torch.min(surr1, surr2).mean() + 0.5*(advantage**2).mean()\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "# ============================================\n",
        "# ğŸ“Œ 11. A2C Agent ì •ì˜\n",
        "# ============================================\n",
        "# - Advantage Actor-Critic\n",
        "# - Baseline(value)ì„ í™œìš©í•˜ì—¬ variance ê°ì†Œ\n",
        "# ============================================\n",
        "\n",
        "class A2CAgent:\n",
        "    def __init__(self, obs_dim, action_dim, lr=1e-3, gamma=0.99):\n",
        "        self.model = ActorCritic(obs_dim, action_dim)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def act(self, obs):\n",
        "        obs = torch.tensor(obs, dtype=torch.float32)\n",
        "        probs, _ = self.model(obs)\n",
        "        dist = Categorical(probs)\n",
        "        action = dist.sample()\n",
        "        return action.item(), dist.log_prob(action)\n",
        "\n",
        "    # A2C ì—…ë°ì´íŠ¸\n",
        "    def update(self, obs, action, reward, logprob):\n",
        "        obs = torch.tensor(obs, dtype=torch.float32)\n",
        "        probs, value = self.model(obs)\n",
        "\n",
        "        advantage = reward - value\n",
        "\n",
        "        actor_loss = -logprob * advantage\n",
        "        critic_loss = advantage.pow(2)\n",
        "\n",
        "        loss = actor_loss + critic_loss\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "# ============================================\n",
        "# ğŸ“Œ 12. Multi-Armed Bandit Agent ì •ì˜\n",
        "# ============================================\n",
        "# - ê°•í™”í•™ìŠµ ì¤‘ ê°€ì¥ ë‹¨ìˆœí•œ epsilon-greedy íƒìƒ‰ ê¸°ë²•\n",
        "# - ë¹„êµ ì‹¤í—˜ìš© baselineìœ¼ë¡œ ì‚¬ìš©\n",
        "# ============================================\n",
        "\n",
        "class BanditAgent:\n",
        "    def __init__(self, n_actions=10, eps=0.1):\n",
        "        self.n_actions = n_actions\n",
        "        self.eps = eps\n",
        "        self.values = np.zeros(n_actions)\n",
        "        self.counts = np.zeros(n_actions)\n",
        "\n",
        "    def act(self):\n",
        "        if np.random.rand() < self.eps:\n",
        "            return np.random.randint(self.n_actions)\n",
        "        return np.argmax(self.values)\n",
        "\n",
        "    def update(self, action, reward):\n",
        "        self.counts[action] += 1\n",
        "        lr = 1 / self.counts[action]\n",
        "        self.values[action] += lr * (reward - self.values[action])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "HiiUSSQ9IF7T"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# ğŸ“Œ 13. ì„¸ ê°€ì§€ ì•Œê³ ë¦¬ì¦˜ í•™ìŠµ í•¨ìˆ˜ ì •ì˜\n",
        "# ============================================\n",
        "# - ê¸°ë³¸ Hyperparameter ì ìš© ì‹œ\n",
        "# ============================================\n",
        "\n",
        "def train_PPO(env, episodes=20):\n",
        "    agent = PPOAgent(768, env.max_candidates)\n",
        "    for ep in range(episodes):\n",
        "        obs, _ = env.reset()\n",
        "\n",
        "        memory = {\"obs\": [], \"actions\": [], \"rewards\": [], \"logprobs\": []}\n",
        "\n",
        "        action, logprob = agent.act(obs)\n",
        "        _, reward, _, _, _ = env.step(action)\n",
        "\n",
        "        memory[\"obs\"].append(torch.tensor(obs))\n",
        "        memory[\"actions\"].append(action)\n",
        "        memory[\"rewards\"].append(reward)\n",
        "        memory[\"logprobs\"].append(logprob)\n",
        "\n",
        "        agent.update(memory)\n",
        "\n",
        "    return agent\n",
        "\n",
        "\n",
        "def train_A2C(env, episodes=20):\n",
        "    agent = A2CAgent(768, env.max_candidates)\n",
        "    for ep in range(episodes):\n",
        "        obs, _ = env.reset()\n",
        "        action, logprob = agent.act(obs)\n",
        "        _, reward, _, _, _ = env.step(action)\n",
        "        agent.update(obs, action, reward, logprob)\n",
        "    return agent\n",
        "\n",
        "\n",
        "def train_Bandit(env, episodes=20):\n",
        "    agent = BanditAgent(env.max_candidates)\n",
        "    for ep in range(episodes):\n",
        "        obs, _ = env.reset()\n",
        "        action = agent.act()\n",
        "        _, reward, _, _, _ = env.step(action)\n",
        "        agent.update(action, reward)\n",
        "    return agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "jRKMcteOJqS-"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# ğŸ“Œ 14. í‰ê°€ í•¨ìˆ˜ ì •ì˜\n",
        "# ============================================\n",
        "# - test_dfë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ì•Œê³ ë¦¬ì¦˜ë³„ accuracy ê³„ì‚°\n",
        "# ============================================\n",
        "\n",
        "def evaluate_agent(agent, df, mode=\"PPO\"):\n",
        "    correct = 0\n",
        "    env = ModelExtractEnv(df)\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        obs, _ = env.reset()\n",
        "\n",
        "        if mode == \"PPO\":\n",
        "            action, _ = agent.act(obs)\n",
        "        elif mode == \"A2C\":\n",
        "            action, _ = agent.act(obs)\n",
        "        else:  # Bandit\n",
        "            action = agent.act()\n",
        "\n",
        "        row = df.iloc[env.index]\n",
        "        labels = row[\"labels\"]\n",
        "        answer = row[\"answer\"]\n",
        "\n",
        "        if action < len(labels) and labels[action] == answer:\n",
        "            correct += 1\n",
        "\n",
        "    return correct / len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Lvug8ncDKRlb"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# ğŸ“Œ 15. Train/Test Split\n",
        "# ============================================\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, test_df = train_test_split(\n",
        "    df, test_size=0.2, shuffle=True, random_state=42\n",
        ")\n",
        "\n",
        "train_env = ModelExtractEnv(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "gqoFZMO5JqVQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63bd9007-3966-4fa7-e4f5-081ba56a283b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Test Accuracy ===\n",
            "PPO:    0.99\n",
            "A2C:    0.62\n",
            "Bandit: 0.91\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# ğŸ“Œ 16. ì„¸ ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ í•™ìŠµ\n",
        "# ============================================\n",
        "\n",
        "ppo_agent = train_PPO(train_env)\n",
        "a2c_agent = train_A2C(train_env)\n",
        "bandit_agent = train_Bandit(train_env)\n",
        "\n",
        "# ============================================\n",
        "# ğŸ“Œ 17. Test Accuracy ë¹„êµ ì¶œë ¥\n",
        "# ============================================\n",
        "\n",
        "ppo_acc = evaluate_agent(ppo_agent, test_df, mode=\"PPO\")\n",
        "a2c_acc = evaluate_agent(a2c_agent, test_df, mode=\"A2C\")\n",
        "bandit_acc = evaluate_agent(bandit_agent, test_df, mode=\"Bandit\")\n",
        "\n",
        "print(\"=== Test Accuracy ===\")\n",
        "print(f\"PPO:    {ppo_acc:.2f}\")\n",
        "print(f\"A2C:    {a2c_acc:.2f}\")\n",
        "print(f\"Bandit: {bandit_acc:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "illg1cJUJqXI"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# ğŸ“Œ 18. ê°•í™”í•™ìŠµ Hyperparameter íƒìƒ‰ (PPO / A2C / Bandit)\n",
        "# ============================================\n",
        "import random\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "def ppo_hyperparameter_search(train_df, test_df):\n",
        "\n",
        "    # íƒìƒ‰í•  hyperparameter í›„ë³´ë“¤\n",
        "    lr_list = [1e-4, 3e-4, 5e-4]\n",
        "    clip_list = [0.1, 0.2, 0.3]\n",
        "    gamma_list = [0.95, 0.99]\n",
        "\n",
        "    best_score = -999\n",
        "    best_params = None\n",
        "\n",
        "    for lr in lr_list:\n",
        "        for clip in clip_list:\n",
        "            for gamma in gamma_list:\n",
        "\n",
        "                print(f\"â–¶ Testing PPO params: lr={lr}, epsilon={clip}, gamma={gamma}\")\n",
        "\n",
        "                env = ModelExtractEnv(train_df)\n",
        "                agent = PPOAgent(\n",
        "                    obs_dim=768,\n",
        "                    action_dim=env.max_candidates,\n",
        "                    lr=lr,\n",
        "                    gamma=gamma,\n",
        "                    eps_clip=clip\n",
        "                )\n",
        "\n",
        "                # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš© ì—í”¼ì†Œë“œ ìˆ˜\n",
        "                for _ in range(10):\n",
        "                    obs, _ = env.reset()\n",
        "                    memory = {\"obs\": [], \"actions\": [], \"rewards\": [], \"logprobs\": []}\n",
        "\n",
        "                    action, logprob = agent.act(obs)\n",
        "                    _, reward, _, _, _ = env.step(action)\n",
        "\n",
        "                    memory[\"obs\"].append(torch.tensor(obs))\n",
        "                    memory[\"actions\"].append(action)\n",
        "                    memory[\"rewards\"].append(reward)\n",
        "                    memory[\"logprobs\"].append(logprob)\n",
        "                    agent.update(memory)\n",
        "\n",
        "                # í‰ê°€\n",
        "                score = evaluate_agent(agent, test_df, mode=\"PPO\")\n",
        "\n",
        "                print(f\"â–¶ Score: {score:.4f}\")\n",
        "\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_params = (lr, clip, gamma)\n",
        "\n",
        "    print(\"\\n===== PPO Best Hyperparameter =====\")\n",
        "    print(f\"Learning rate   = {best_params[0]}\")\n",
        "    print(f\"Clip epsilon    = {best_params[1]}\")\n",
        "    print(f\"Gamma           = {best_params[2]}\")\n",
        "    print(f\"Best Test Acc   = {best_score:.4f}\")\n",
        "\n",
        "    return best_params\n",
        "\n",
        "def a2c_hyperparameter_search(train_df, test_df):\n",
        "\n",
        "    lr_list = [1e-4, 5e-4, 1e-3]\n",
        "    gamma_list = [0.95, 0.99]\n",
        "\n",
        "    best_score = -999\n",
        "    best_params = None\n",
        "\n",
        "    for lr in lr_list:\n",
        "        for gamma in gamma_list:\n",
        "\n",
        "            print(f\"â–¶ Testing A2C params: lr={lr}, gamma={gamma}\")\n",
        "\n",
        "            env = ModelExtractEnv(train_df)\n",
        "            agent = A2CAgent(768, env.max_candidates, lr=lr, gamma=gamma)\n",
        "\n",
        "            for _ in range(10):\n",
        "                obs, _ = env.reset()\n",
        "                action, logprob = agent.act(obs)\n",
        "                _, reward, _, _, _ = env.step(action)\n",
        "                agent.update(obs, action, reward, logprob)\n",
        "\n",
        "            score = evaluate_agent(agent, test_df, mode=\"A2C\")\n",
        "\n",
        "            print(f\"â–¶ Score: {score:.4f}\")\n",
        "\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_params = (lr, gamma)\n",
        "\n",
        "    print(\"\\n===== A2C Best Hyperparameter =====\")\n",
        "    print(f\"Learning rate   = {best_params[0]}\")\n",
        "    print(f\"Gamma           = {best_params[1]}\")\n",
        "    print(f\"Best Test Acc   = {best_score:.4f}\")\n",
        "\n",
        "    return best_params\n",
        "\n",
        "def bandit_hyperparameter_search(train_df, test_df):\n",
        "\n",
        "    eps_list = [0.05, 0.1, 0.2]\n",
        "\n",
        "    best_score = -999\n",
        "    best_params = None\n",
        "\n",
        "    for eps in eps_list:\n",
        "\n",
        "        print(f\"â–¶ Testing Bandit eps={eps}\")\n",
        "\n",
        "        env = ModelExtractEnv(train_df)\n",
        "        agent = BanditAgent(env.max_candidates, eps=eps)\n",
        "\n",
        "        for _ in range(10):\n",
        "            obs, _ = env.reset()\n",
        "            action = agent.act()\n",
        "            _, reward, _, _, _ = env.step(action)\n",
        "            agent.update(action, reward)\n",
        "\n",
        "        score = evaluate_agent(agent, test_df, mode=\"Bandit\")\n",
        "\n",
        "        print(f\"â–¶ Score: {score:.4f}\")\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_params = eps\n",
        "\n",
        "    print(\"\\n===== Bandit Best Hyperparameter =====\")\n",
        "    print(f\"Epsilon         = {best_params}\")\n",
        "    print(f\"Best Test Acc   = {best_score:.4f}\")\n",
        "\n",
        "    return best_params"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "\n",
        "best_ppo = ppo_hyperparameter_search(train_df, test_df)\n",
        "best_a2c = a2c_hyperparameter_search(train_df, test_df)\n",
        "best_bandit = bandit_hyperparameter_search(train_df, test_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V56fkiZ4k7fp",
        "outputId": "f17085b7-5836-45aa-9b28-58105e122223"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â–¶ Testing PPO params: lr=0.0001, epsilon=0.1, gamma=0.95\n",
            "â–¶ Score: 0.0863\n",
            "â–¶ Testing PPO params: lr=0.0001, epsilon=0.1, gamma=0.99\n",
            "â–¶ Score: 0.1314\n",
            "â–¶ Testing PPO params: lr=0.0001, epsilon=0.2, gamma=0.95\n",
            "â–¶ Score: 0.2338\n",
            "â–¶ Testing PPO params: lr=0.0001, epsilon=0.2, gamma=0.99\n",
            "â–¶ Score: 0.1506\n",
            "â–¶ Testing PPO params: lr=0.0001, epsilon=0.3, gamma=0.95\n",
            "â–¶ Score: 0.1387\n",
            "â–¶ Testing PPO params: lr=0.0001, epsilon=0.3, gamma=0.99\n",
            "â–¶ Score: 0.1705\n",
            "â–¶ Testing PPO params: lr=0.0003, epsilon=0.1, gamma=0.95\n",
            "â–¶ Score: 0.3661\n",
            "â–¶ Testing PPO params: lr=0.0003, epsilon=0.1, gamma=0.99\n",
            "â–¶ Score: 0.2088\n",
            "â–¶ Testing PPO params: lr=0.0003, epsilon=0.2, gamma=0.95\n",
            "â–¶ Score: 0.2339\n",
            "â–¶ Testing PPO params: lr=0.0003, epsilon=0.2, gamma=0.99\n",
            "â–¶ Score: 0.1971\n",
            "â–¶ Testing PPO params: lr=0.0003, epsilon=0.3, gamma=0.95\n",
            "â–¶ Score: 0.6408\n",
            "â–¶ Testing PPO params: lr=0.0003, epsilon=0.3, gamma=0.99\n",
            "â–¶ Score: 0.8067\n",
            "â–¶ Testing PPO params: lr=0.0005, epsilon=0.1, gamma=0.95\n",
            "â–¶ Score: 0.2842\n",
            "â–¶ Testing PPO params: lr=0.0005, epsilon=0.1, gamma=0.99\n",
            "â–¶ Score: 0.6132\n",
            "â–¶ Testing PPO params: lr=0.0005, epsilon=0.2, gamma=0.95\n",
            "â–¶ Score: 0.8650\n",
            "â–¶ Testing PPO params: lr=0.0005, epsilon=0.2, gamma=0.99\n",
            "â–¶ Score: 0.8913\n",
            "â–¶ Testing PPO params: lr=0.0005, epsilon=0.3, gamma=0.95\n",
            "â–¶ Score: 0.8361\n",
            "â–¶ Testing PPO params: lr=0.0005, epsilon=0.3, gamma=0.99\n",
            "â–¶ Score: 0.1195\n",
            "\n",
            "===== PPO Best Hyperparameter =====\n",
            "Learning rate   = 0.0005\n",
            "Clip epsilon    = 0.2\n",
            "Gamma           = 0.99\n",
            "Best Test Acc   = 0.8913\n",
            "â–¶ Testing A2C params: lr=0.0001, gamma=0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_confidence_interval(data, confidence=0.95):\n",
        "    data = np.array(data)\n",
        "    mean = np.mean(data)\n",
        "    std = np.std(data)\n",
        "    n = len(data)\n",
        "\n",
        "    margin = 1.96 * (std / np.sqrt(n))  # 95% CI\n",
        "    return mean, margin\n",
        "\n",
        "def evaluate_with_seeds(\n",
        "    train_df, test_df,\n",
        "    algo=\"PPO\",\n",
        "    params=None,\n",
        "    seed_list=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "):\n",
        "\n",
        "    accuracies = []\n",
        "\n",
        "    for seed in seed_list:\n",
        "        set_seed(seed)\n",
        "\n",
        "        env = ModelExtractEnv(train_df)\n",
        "\n",
        "        if algo == \"PPO\":\n",
        "            agent = PPOAgent(\n",
        "                obs_dim=768,\n",
        "                action_dim=env.max_candidates,\n",
        "                lr=params[\"lr\"],\n",
        "                gamma=params[\"gamma\"],\n",
        "                eps_clip=params[\"clip\"],\n",
        "            )\n",
        "\n",
        "            # ë¹ ë¥¸ ì‹¤í—˜ìš© mini training (í•„ìš”í•˜ë©´ episodes ëŠ˜ë¦¼)\n",
        "            for _ in range(10):\n",
        "                obs, _ = env.reset()\n",
        "                memory = {\"obs\": [], \"actions\": [], \"rewards\": [], \"logprobs\": []}\n",
        "                action, logprob = agent.act(obs)\n",
        "                _, reward, _, _, _ = env.step(action)\n",
        "                memory[\"obs\"].append(torch.tensor(obs))\n",
        "                memory[\"actions\"].append(action)\n",
        "                memory[\"rewards\"].append(reward)\n",
        "                memory[\"logprobs\"].append(logprob)\n",
        "                agent.update(memory)\n",
        "\n",
        "        elif algo == \"A2C\":\n",
        "            agent = A2CAgent(\n",
        "                768, env.max_candidates,\n",
        "                lr=params[\"lr\"],\n",
        "                gamma=params[\"gamma\"]\n",
        "            )\n",
        "\n",
        "            for _ in range(10):\n",
        "                obs, _ = env.reset()\n",
        "                action, logprob = agent.act(obs)\n",
        "                _, reward, _, _, _ = env.step(action)\n",
        "                agent.update(obs, action, reward, logprob)\n",
        "\n",
        "        elif algo == \"Bandit\":\n",
        "            agent = BanditAgent(\n",
        "                n_actions=env.max_candidates,\n",
        "                eps=params[\"eps\"]\n",
        "            )\n",
        "\n",
        "            for _ in range(10):\n",
        "                obs, _ = env.reset()\n",
        "                action = agent.act()\n",
        "                _, reward, _, _, _ = env.step(action)\n",
        "                agent.update(action, reward)\n",
        "\n",
        "        # í…ŒìŠ¤íŠ¸ ì„±ëŠ¥\n",
        "        acc = evaluate_agent(agent, test_df, mode=algo)\n",
        "        accuracies.append(acc)\n",
        "\n",
        "    # í‰ê·  + 95% ì‹ ë¢°êµ¬ê°„ ê³„ì‚°\n",
        "    mean, ci = mean_confidence_interval(accuracies)\n",
        "    return mean, ci, accuracies"
      ],
      "metadata": {
        "id": "lvj2610-k7b7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fgfEGt5XJqYd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "371373c2-9068-4c49-821d-3277a5f97221"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-776466440.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbest_bandit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"eps\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mppo_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppo_ci\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppo_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_with_seeds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"PPO\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_ppo_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0ma2c_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma2c_ci\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma2c_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_with_seeds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"A2C\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_a2c_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mbandit_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbandit_ci\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbandit_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_with_seeds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Bandit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_bandit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
          ]
        }
      ],
      "source": [
        "best_ppo_params = {\"lr\": 0.0005, \"clip\": 0.3, \"gamma\": 0.95}\n",
        "best_a2c_params = {\"lr\": 1e-3, \"gamma\": 0.95}\n",
        "best_bandit_params = {\"eps\": 0.05}\n",
        "\n",
        "ppo_mean, ppo_ci, ppo_all = evaluate_with_seeds(train_df, test_df, \"PPO\", best_ppo_params)\n",
        "a2c_mean, a2c_ci, a2c_all = evaluate_with_seeds(train_df, test_df, \"A2C\", best_a2c_params)\n",
        "bandit_mean, bandit_ci, bandit_all = evaluate_with_seeds(train_df, test_df, \"Bandit\", best_bandit_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D47rL7noIF-g"
      },
      "outputs": [],
      "source": [
        "print(\"===== RL ëª¨ë¸ Seed ê¸°ë°˜ ì‹ ë¢°êµ¬ê°„ í‰ê°€ =====\")\n",
        "\n",
        "print(f\"PPO    : mean={ppo_mean:.3f}, 95% CI=Â±{ppo_ci:.3f}\")\n",
        "print(f\"A2C    : mean={a2c_mean:.3f}, 95% CI=Â±{a2c_ci:.3f}\")\n",
        "print(f\"Bandit : mean={bandit_mean:.3f}, 95% CI=Â±{bandit_ci:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# ì‹¤í—˜ ê²°ê³¼ ì…ë ¥\n",
        "means = [ppo_mean, a2c_mean, bandit_mean]\n",
        "cis = [ppo_ci, a2c_ci, bandit_ci]\n",
        "models = [\"PPO\", \"A2C\", \"Bandit\"]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.errorbar(models, means, yerr=cis, fmt='o', capsize=8, markersize=8)\n",
        "plt.title(\"Model Accuracy with 95% Confidence Interval\")\n",
        "plt.xlabel(\"Algorithm\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.grid(True, linestyle='--', alpha=0.4)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fgTzbnQGojnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4L9lDR7Lojpt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1MlPSvM-RmXBPC6ICIcxSm6PcbxIX3gS6",
      "authorship_tag": "ABX9TyOWYhyQ9cswS4ed6YtYhcfC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}